{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf-_uA8P5Hf7"
      },
      "source": [
        "Assignment 1:\n",
        "\n",
        "Name: Nishad Wajge\n",
        "\n",
        "UID: 119578728\n",
        "\n",
        "Please submit to Gradescope\n",
        "- a PDF containing all outputs (by executing **Run all**).\n",
        "- your ipynb notebook containing all the code\n",
        "\n",
        "I understand the policy on academic integraty (collaboration and the use of online material).\n",
        "Please sign your name here: Nishad Wajge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ1mbp_X5zfn"
      },
      "source": [
        "There are both coding problems and write-up problems in this assignment.\n",
        "\n",
        "For coding problems, implement functions and scripts where you see a `TODO` or parts with points.\n",
        "\n",
        "For write-ups, make sure you answer all the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKDbGxBtmGFz"
      },
      "source": [
        "# Part A: Image Colorization (55 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX5gyKRJmk_m"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The goal of Part A is to reconstruct a full-color image from the digitized Prokudin-Gorskii glass plate photographs, which consist of three grayscale images captured through red, green, and blue filters. These three channel images are vertically stacked in a single image and are slightly misaligned due to camera movement and subject motion during exposure.\n",
        "\n",
        "To produce a visually coherent color image, the three color channels must first be extracted and then precisely aligned with one another before being combined into a single RGB image. The primary challenge lies in accurately aligning the channels so as to minimize visual artifacts. This alignment is performed by searching over possible displacements and selecting the one that maximizes a similarity metric between channels, such as Sum of Squared Differences (SSD) or Normalized Cross-Correlation (NCC). Once the optimal alignment is found, the channels are stacked to form the final color image.\n",
        "\n",
        "\n",
        "NOTE:\n",
        "\n",
        "1. Reading [this](https://www.loc.gov/exhibits/empire/making.html) for more background information and samples.\n",
        "\n",
        "2. You can use any image processing libraries of your choice such as skimage or cv2; in python.\n",
        "\n",
        "\n",
        "You are required to provide **THREE colorized image results**. Choose ONE of the results, and use it to provide answers in the write-up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FDvZKX3pvGR"
      },
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uWzKA6i68ls",
        "outputId": "aab67050-4a55-4a92-8bf5-1b435145bc0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KTDxPAkQam29YKtoX5dKPnLKpUOWCanC\n",
            "To: /content/hybrid_pyramid_input.zip\n",
            "\r  0% 0.00/2.19M [00:00<?, ?B/s]\r100% 2.19M/2.19M [00:00<00:00, 146MB/s]\n",
            "Archive:  /content/hybrid_pyramid_input.zip\n",
            "replace /content/data/Afghan_girl_before.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1KTDxPAkQam29YKtoX5dKPnLKpUOWCanC\n",
        "!unzip \"/content/hybrid_pyramid_input.zip\" -d \"/content/\"\n",
        "\n",
        "!gdown 19_UNxJ6zAH5-4-ljo-OvnuLKWhtkaChO\n",
        "!unzip \"/content/parta_data.zip\" -d \"/content/parta_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tCWRXY120Hm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('/content/data'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MTbv0W43jU8"
      },
      "source": [
        "##A.0 Know your data (20 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygoi99vJJLB2"
      },
      "source": [
        "### Read and show images **(10 pts)**\n",
        "Let's start with some helper functions. Implement helper functions below so you can read and visualize images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohr1bQY53gD6"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages here\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Helper Functions\n",
        "def read_image_pil(image_path: str) -> Image:\n",
        "    \"\"\"\n",
        "    :param image_path: path to the image\n",
        "    :return: representation of the image\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE (10 pts)\n",
        "    # Read an image using PIL.Image.open()\n",
        "    image = Image.open(image_path)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk2Mv_B84HS4"
      },
      "outputs": [],
      "source": [
        "# Check if you can read and show an image. You are free to change path.\n",
        "image_path = '/content/data/cat.bmp'\n",
        "image_pil = read_image_pil(image_path)\n",
        "# PIL image can be directly shown by just calling it.\n",
        "image_pil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jd0oy3R5QLG"
      },
      "source": [
        "We can do conversions between PIL image format and numpy array. Try commands below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaSdsEay5FO3"
      },
      "outputs": [],
      "source": [
        "print(image_pil)  # show the data type of an PIL image\n",
        "image_np = np.array(image_pil) # PIL image -> numpy array\n",
        "print('Image shape: ', image_np.shape) # show the shape\n",
        "print('Data type: ', image_np.dtype) # show the data type\n",
        "print(image_np) # show exact pixel values\n",
        "image_pil_new = Image.fromarray(image_np) # numpy array -> PIL image\n",
        "print(image_pil_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGlMbIqd81W2"
      },
      "source": [
        "There are also other ways to show an image. For example, you can also use `matplotlib.pyplot`. Run the code below and get yourself familiar with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBKeJrrD80oW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(image_np)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9wwjR2G6s0z"
      },
      "source": [
        "###Write-up (10 pts)\n",
        "1. What is the difference between a `PIL` image and an image as a `numpy.array`? **(5 pt)**\n",
        "2. What is the data type of an image by default it is directly converted from a `PIL` image? What is the value range?  **(5 pts)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIscmzcjJQDU"
      },
      "source": [
        "**Include your write-up here**\n",
        "1. The difference is that a PIL image is an object that has metadata like format size, channels, etc.. A numpy array represents the image as a tensor of pixel values. PIL images have specific operations such as resize, rotate, and save, whereas numpy arrays have math ops like multiplication, slicing, and analysis. PIL is good for file I/O and numpy is best for manipulating pixel values.\n",
        "2. The value type for PIL is uint8, with a value range from 0 to 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgwzdl7OFLG4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PHPoPsDroQE"
      },
      "source": [
        "##A.1 Implementation of Image Colorization (35 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LPV_cYF-gOS"
      },
      "source": [
        "###Data type **(5 pts)**\n",
        "In practice, we need to convert the data type into `float` so that we can apply mathmatical operations. We first write `read_image` function below. Let's use `float64` in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFkK55MWtCX6"
      },
      "outputs": [],
      "source": [
        "def read_image(image_path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param image_path: path to the image\n",
        "    :return: floating representation of the image. Use np.float64.\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE (10 pts)\n",
        "    image = np.array(Image.open(image_path), dtype=np.float64)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMjqo91e9zQi"
      },
      "source": [
        "Let's check if you are doing it right. The value range should be in [0.0, 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc6y9IW49yp_"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/parta_data/00125v.jpg'\n",
        "img = read_image(image_path)\n",
        "print(img.shape)\n",
        "print(img.dtype)\n",
        "print('value range: [{}, {}]'.format(img.min(), img.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCVoMi3TuacL"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYvgCQVar9Io"
      },
      "outputs": [],
      "source": [
        "def read_and_split_image(image_path: str) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    NO NEED TO CHANGE THIS FUNCTION.\n",
        "    Reads an image and splits it into its RGB channels.\n",
        "    :param image_path: path to the image\n",
        "    :return: R, G, B data\n",
        "    \"\"\"\n",
        "    # read in the image\n",
        "    im = read_image(image_path)\n",
        "    # compute the height of each part (just 1/3 of total)\n",
        "    height = np.floor(im.shape[0] / 3.0).astype(int)\n",
        "\n",
        "    # separate color channels\n",
        "    b = im[:height]\n",
        "    g = im[height:2 * height]\n",
        "    r = im[2 * height:3 * height]\n",
        "\n",
        "    return r, g, b\n",
        "\n",
        "def center_crop(img: np.ndarray, frac: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    NO NEED TO CHANGE THIS FUNCTION.\n",
        "    Return the centered crop of an image. frac=0.5 keep the central 50% in both height and width.\n",
        "    :param img: image array\n",
        "    :return: cropped image\n",
        "    \"\"\"\n",
        "    h, w = img.shape\n",
        "    ch, cw = int(h * frac), int(w * frac)\n",
        "    top = (h - ch) // 2\n",
        "    left = (w - cw) // 2\n",
        "    return img[top:top+ch, left:left+cw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNrq9uB_78AL"
      },
      "source": [
        "###Normalized Cross-Correlation (NCC) (5 pts)\n",
        "\n",
        "NCC measures the similarity between two images while being robust to differences in brightness and contrast. Since the Prokudin-Gorskii color channels were captured using different filters and exposure conditions, raw pixel differences can be misleading. NCC addresses this by normalizing both images to zero mean and unit variance before computing their similarity.\n",
        "\n",
        "Given two image patches A and B, NCC is computed as:\n",
        "$$NCC(A,B)= \\frac{1}{N} \\sum ( \\frac{A-\\mu_A}{\\sigma_a})( \\frac{B-\\mu_B}{\\sigma_B}) \\ $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_7uli_gudmO"
      },
      "outputs": [],
      "source": [
        "def normalized_cross_correlation(img1: np.ndarray, img2: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute the normalized cross-correlation (NCC) between two arrays.\n",
        "    :param img1, img2: image array\n",
        "    :return score\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE (5 pts)\n",
        "    # Step 1: get the norm of each image\n",
        "    img1_norm = np.linalg.norm(img1)\n",
        "    img2_norm = np.linalg.norm(img2)\n",
        "\n",
        "    # Step 1: compute ncc score from two normalized images\n",
        "    ncc = np.sum(img1 * img2) / (img1_norm * img2_norm)\n",
        "\n",
        "    return ncc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoFhxmTx96WT"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "\n",
        "img = np.random.randn(100, 100)\n",
        "score = normalized_cross_correlation(img, img)\n",
        "assert score > 0.99, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au_TvbGm_Q5S"
      },
      "source": [
        "###Image Alignment (5 pts)\n",
        "\n",
        "This function aligns one image to a reference image by exhaustively searching over a fixed window of possible shifts. For each candidate displacement, the moving image is shifted, and NCC is computed over the center crop. The displacement that yields the highest NCC score is selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMdr5HKD91YE"
      },
      "outputs": [],
      "source": [
        "def align_images(mov: np.ndarray, ref: np.ndarray, max_shift: int = 32) -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Align mov to ref by searching over a window of shifts and maximizing NCC.\n",
        "\n",
        "    :param ref: reference image (e.g., blue channel)\n",
        "    :param mov: moving image (e.g., red or green channel)\n",
        "    :param max_shift: maximum pixel displacement in each direction\n",
        "    :return: (dx, dy) shift that best aligns mov to ref\n",
        "    \"\"\"\n",
        "\n",
        "    best_dx, best_dy = 0, 0\n",
        "    best_score = -np.inf\n",
        "\n",
        "    ref_c = center_crop(ref)\n",
        "\n",
        "    # TODO: YOUR CODE HERE (5 pts)\n",
        "    for dx in range(-max_shift, max_shift + 1):\n",
        "        for dy in range(-max_shift, max_shift + 1):\n",
        "            # Shift mov by (dx, dy), hint: use np.roll\n",
        "            shifted_mov_x = np.roll(mov, shift=dx, axis=1) # Shift mov by dx in x dim\n",
        "            shifted_mov = np.roll(shifted_mov_x, shift=dy, axis=0) # Shift shifted_mov_x by dy in y dim\n",
        "\n",
        "            # Compute NCC on the centered image to avoid edge effects\n",
        "            shifted_c = center_crop(shifted_mov) # get the centered shifted_mov\n",
        "            score = normalized_cross_correlation(shifted_c, ref_c)\n",
        "\n",
        "            # Update best shift\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_dx, best_dy = dx, dy\n",
        "\n",
        "    return best_dx, best_dy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMxTzx4y-_mE"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "np.random.seed(0)\n",
        "ref = np.random.randn(120, 160)\n",
        "true_dx, true_dy = 8, -5\n",
        "mov = np.roll(ref, true_dx, axis=1)\n",
        "mov = np.roll(mov, true_dy, axis=0)\n",
        "\n",
        "dx, dy = align_images(ref, mov, max_shift=15)\n",
        "assert (dx, dy) == (true_dx, true_dy), (dx, dy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxGNkgUtyFEg"
      },
      "source": [
        "### Image Colorization (10 pts)\n",
        "It's time to officially implement Image Colorization.\n",
        "\n",
        "Let's wrap up all the stuff in a single function `image_colorization`. The key idea is to choose one channel as the reference, then align the remaining two channels to this reference using the alignment function implemented earlier. In the provided hint code, the blue channel is used as the reference, but you are encouraged to experiment with using the red or green channel instead and compare the visual results.\n",
        "\n",
        "After computing the optimal shifts, apply them to align the channels. Finally, stack the aligned red, green, and blue channels together to form the reconstructed RGB color image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVssHru2sMNg"
      },
      "outputs": [],
      "source": [
        "def image_colorization(image_path: str):\n",
        "    \"\"\"\n",
        "    :param image_path: path to the image\n",
        "    :return: Colorized image\n",
        "    \"\"\"\n",
        "    r, g, b = read_and_split_image(image_path)\n",
        "\n",
        "    # TODO: YOUR CODE HERE (5 pts)\n",
        "    # Step 1: get the best shifts\n",
        "    dx_r, dy_r = align_images(r, b) # shift red channel take blue channel as reference\n",
        "    dx_g, dy_g = align_images(g, b) # shift green channel take blue channel as reference\n",
        "\n",
        "    # Step 2: apply the shifts, hint: use np.roll\n",
        "    r_shifted = np.roll(np.roll(r, dx_r, axis=1), dy_r, axis=0)\n",
        "    g_shifted = np.roll(np.roll(g, dx_g, axis=1), dy_g, axis=0)\n",
        "    b_shifted = b\n",
        "\n",
        "    # Step 3: create a color image, hint: use np.stack\n",
        "    color_img = np.stack((r_shifted, g_shifted, b_shifted), axis=-1)\n",
        "\n",
        "    return color_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwGJYp_eyqgt"
      },
      "source": [
        "Let's see if we are doing it right. You should see the colorized image like [This](https://drive.google.com/file/d/1qSLrMnNfwoQKOPXrhDhW-y7HYQ2kz-F6/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY4LAuDEywPQ"
      },
      "outputs": [],
      "source": [
        "colorized_img = image_colorization('/content/parta_data/00398v.jpg')\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(colorized_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIosuEYTzBDY"
      },
      "source": [
        "### Write-up (10 pts)\n",
        "\n",
        "\n",
        "1.   Run `image_colorization` on 3 different images. Report your results below. (5 pts)\n",
        "4.   Briefly explain how this works, using your favorite results as illustrations. (5 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJB8drG8FH6r"
      },
      "source": [
        "**Include your write-up here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFX9HqPfFNzx"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ftoiwAo45n5"
      },
      "source": [
        "# Part B: Hybrid Image (45 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJikaY9ICDYH"
      },
      "source": [
        "## Overview\n",
        "\n",
        "A hybrid image is the sum of a *low-pass filtered* version of the one image and a *high-pass filtered* version of a second image. There is a free parameter, which can be tuned for each image pair, which controls how much high frequency to remove from the first image and how much low frequency to leave in the second image. This is called the “cutoff-frequency”. In the paper it is suggested to use two cutoff frequencies (one tuned for each image) and you are free to try that, as well. In the starter code, the cutoff frequency is controlled by changing the standard deviation of the Gausian filter used in constructing the hybrid images. [This](https://drive.google.com/uc?id=187FjBJLwnYXhylx08Vdh1SAA3AO-imYv) is the sample example.\n",
        "\n",
        "NOTE:\n",
        "\n",
        "1. Reading [this](https://stanford.edu/class/ee367/reading/OlivaTorralb_Hybrid_Siggraph06.pdf) will help in understanding Part B.\n",
        "\n",
        "2. You can use any image processing libraries of your choice such as skimage or cv2; in python.\n",
        "\n",
        "We provided 7 pairs of aligned images. The alignment is important because it affects the perceptual grouping (read the paper for details). We encourage you to create additional examples (e.g. change of expression, morph between different objects, change over time, etc.).\n",
        "\n",
        "You are required to provide **THREE hybrid image results**. Choose ONE of the results, and use it to provide answers in the write-up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMncKsSZ7z9F"
      },
      "source": [
        "##B.1 Implementation of Hybrid Image (55 pts)\n",
        "Now let's start to implement hybrid image. We will start again with some helper functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-U4_pgvsvrt"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0A9GNBX9di2"
      },
      "outputs": [],
      "source": [
        "def vis_hybrid_image(hybrid_image) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  NO NEED TO CHANGE THIS FUNCTION.\n",
        "  Visualize a hybrid image by progressively downsampling the image and\n",
        "  concatenating all of the images together.\n",
        "  :param hybrid_image:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  scales = 5\n",
        "  scale_factor = 0.5\n",
        "  padding = 5\n",
        "  original_height = hybrid_image.shape[0]\n",
        "  num_colors = hybrid_image.shape[2] # counting how many color channels the input has\n",
        "  output = hybrid_image\n",
        "  cur_image = hybrid_image\n",
        "\n",
        "  for i in range(2, scales):\n",
        "      # add padding\n",
        "      output = np.concatenate((output, np.ones((original_height, padding, num_colors), dtype=int)), axis=1)\n",
        "      # dowsample image;\n",
        "      width = int(cur_image.shape[1] * scale_factor)\n",
        "      height = int(cur_image.shape[0] * scale_factor)\n",
        "      dim = (width, height)\n",
        "      cur_image = cv2.resize(cur_image, dim, interpolation = cv2.INTER_LINEAR)\n",
        "      # pad the top and append to the output\n",
        "      tmp = np.concatenate((np.ones((original_height-cur_image.shape[0], cur_image.shape[1], num_colors)), cur_image), axis=0)\n",
        "      output = np.concatenate((output, tmp), axis=1)\n",
        "\n",
        "  output = (output * 255).astype(np.uint8)\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmO-CW1-8De"
      },
      "source": [
        "###2D Gaussian filters **(15 pts)**\n",
        "We will implement a 2D Gaussian filter function in `gaussian_2D_filter`. **(12 pts)**\n",
        "\n",
        "Recall from our lectures that a 2D Gaussian filter is a matrix/tensor filled with values, represented as:\n",
        "$$G(x,y)= \\frac{1}{2 \\pi \\sigma^2} \\exp^{-\\frac{x^2 + y^2}{2 \\sigma^2}} \\,,$$\n",
        "where $x$ and  $y$  denote the distances from the origin along the horizontal and vertical axes, respectively, and  $\\sigma$  represents the standard deviation, which controls the extent of the blur or \"cut-off frequency\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCFqA1oH2Gy"
      },
      "source": [
        "Hint 1: You probably want to get those coordinates $(x,y)$ so that you can compute values at each pixel. In practice, it is easier to get the coordinates of pixels with `np.meshgrid`. Check out an example [here](https://numpy.org/doc/2.1/reference/generated/numpy.meshgrid.html) (See Examples).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgFmSKzKIaPi"
      },
      "source": [
        "Hint 2: In `numpy`, most of the functions are element-wise, meaning they apply the same operation to every element in an array, e.g., `np.exp`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR29caAOHBt2"
      },
      "source": [
        "To apply a filter to an image, implement `imgfilter`. You can use `scipy.signal.convolve2d` (check out an example [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html)) .**(3 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeXJU9eHmzMb"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "def gaussian_2D_filter(size: tuple[int, int], sigma: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param size: tuple (width, height) that decides the filter size\n",
        "    :param sigma: standard deviation, hyperparameter to control the variance of the filter\n",
        "    :return: 2D gaussian filter with the desired size, and variance scaled by cutoff_frequency\n",
        "    Hint: make sure the filter sums up to one\n",
        "    Do NOT use scipy's API to get the filter. Please just use numpy to implement the Gaussian equation.\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE (5 pts)\n",
        "    # Step 1: Make a mesh grid using `np.meshgrid`. (Why?)\n",
        "\n",
        "\n",
        "    # Step 2: Compute Gaussian function with `np.exp` and the mesh grid.\n",
        "\n",
        "\n",
        "    # Step 3: Normalize so that sum equals 1\n",
        "\n",
        "\n",
        "    return filter\n",
        "\n",
        "def imgfilter(image: np.ndarray, filter: np.ndarray, conv_mode='same', boundary='symm') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply a 2D filter to an image using convolution. Supports both grayscale and RGB images.\n",
        "\n",
        "    :param image: input image (grayscale or RGB) to apply the filter on\n",
        "    :param filter: the filter to apply on the image\n",
        "    :param conv_mode: 'same' or 'valid'\n",
        "    :param boundary: 'symm', 'reflect', or 'wrap'\n",
        "    :return: apply the filter by convolving\n",
        "    Do NOT use for loops. See how to convolve with scipy.signal.convolve2d.\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE (5 pts)\n",
        "\n",
        "    # Step 1: check if it is a grayscale image or a RGB image.\n",
        "\n",
        "    # Step 2: apply the filter to it\n",
        "\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLwLawRq3eyx"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "filter_example = gaussian_2D_filter((25, 25), sigma=5.0)\n",
        "print(filter_example.shape)\n",
        "print('Sum of the filter should be 1: ', filter_example.sum())\n",
        "\n",
        "# Visualize it\n",
        "plt.imshow(filter_example)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezGcMzobJfG8"
      },
      "source": [
        "###Visualize FFT (10 pts)\n",
        "After applying a Gaussian filter, high-frequency component is reduced. A way to check one's frequency domain is to use Fast Fourier Transform (FFT). Let's now implement `log_mag_FFT` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt6puyVA_16u"
      },
      "outputs": [],
      "source": [
        "def log_mag_FFT(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    :param image: float matrix representation of the image\n",
        "    :return: log of the magnitude of the FFT of the image\n",
        "    HINT: You may use np.log(np.abs(np.fft.fftshift(np.fft.fft2(image)))) to achieve it.\n",
        "    NOTE1: numpy fft2 would require you to convert the image to grayscale for it to work properly.\n",
        "    NOTE2: To make grayscale, you may use either `grey = R*0.3 + G*0.59 + B*0.11` or `cv2.cvtColor`.\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "    # Step 1: convert an image to grayscale if needed.\n",
        "\n",
        "    # Step 2: compute log magnitute.\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t2FkmwyKsyc"
      },
      "source": [
        "Let's see how we have done so far. Below we read an image, create and apply a Gaussian filter to it, and visualize the log magnitute of the FFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDswSy6HKsT3"
      },
      "outputs": [],
      "source": [
        "# Read an image\n",
        "img_path = '/content/data/cat.bmp'\n",
        "img = read_image(img_path)\n",
        "\n",
        "# make a Gaussian filter. Feel free to modify parameters.\n",
        "filter_size = 25\n",
        "sigma = 5.0\n",
        "gaussian_filter = gaussian_2D_filter((filter_size, filter_size), sigma)\n",
        "\n",
        "# apply the filter\n",
        "img_low_freq = imgfilter(img, gaussian_filter)\n",
        "\n",
        "# visualize images (before and after, side by side)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_low_freq)\n",
        "plt.show()\n",
        "\n",
        "# visualize the FFT (before and after, side by side)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(log_mag_FFT(img))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(log_mag_FFT(img_low_freq))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrbZFCtBNRFG"
      },
      "source": [
        "### Hybrid image (10 pts)\n",
        "It's time to officially implement Hybrid Image. You're expected to compose two images together.\n",
        "\n",
        "Let's wrap up all the stuff in a single function `hybrid_image`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdI8fVIjOKdX"
      },
      "outputs": [],
      "source": [
        "from re import I\n",
        "# Finish hybrid_image.\n",
        "def hybrid_image(image1: np.ndarray, image2: np.ndarray, sigma_low_freq: int, sigma_high_freq: int, if_visual=False):\n",
        "    \"\"\"\n",
        "    :param image1: image of type float64\n",
        "    :param image2: image of type float64\n",
        "    :param sigma_low_freq: Standard deviation for the low-pass filter (image1)\n",
        "    :param sigma_high_freq: Standard deviation for the high-pass filter (image2)\n",
        "    :return: Hybrid image combining low-frequency and high-frequency content\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine filter size based on sigma (typically 6*sigma covers most of the Gaussian)\n",
        "    filter_size_low = (int(6 * sigma_low_freq) | 1, int(6 * sigma_low_freq) | 1)\n",
        "    filter_size_high = (int(6 * sigma_high_freq) | 1, int(6 * sigma_high_freq) | 1)\n",
        "\n",
        "    ###### TODO START #######\n",
        "    # TODO 1: Finish hybrid image.\n",
        "    # Step 1: Create Gaussian filters\n",
        "    low_pass_filter = ???\n",
        "    high_pass_filter = ???\n",
        "\n",
        "    # Step 2: Apply low-pass filter to image1\n",
        "    low_frequencies = ???\n",
        "\n",
        "    # Step 3: Apply low-pass filter to image2 and subtract from original to get high frequencies\n",
        "    low_from_image2 = ???\n",
        "    high_frequencies = ???\n",
        "\n",
        "    # Step 4: Combine the two components\n",
        "    hybrid = ???\n",
        "\n",
        "    # Step 5: clip output value range. See np.clip().\n",
        "    output = ???\n",
        "\n",
        "    ###### TODO END ######\n",
        "\n",
        "    if if_visual:\n",
        "      # Visualization code. NO NEED TO CHANGE.\n",
        "      # visualize two input images, sigma_low_freq, sigma_high_freq, and the hybrid image.\n",
        "      fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
        "      axs[0, 0].imshow(image1)\n",
        "      axs[0, 1].imshow(low_frequencies)\n",
        "      axs[0, 2].imshow(image2)\n",
        "      axs[0, 3].imshow(high_frequencies + 0.5)\n",
        "      axs[0, 4].imshow(output)\n",
        "\n",
        "      # visualize log magnitude of Fourier Transform of the above.\n",
        "      axs[1, 0].imshow(log_mag_FFT(image1), vmin=0, vmax=10)\n",
        "      axs[1, 1].imshow(log_mag_FFT(low_frequencies), vmin=0, vmax=10)\n",
        "      axs[1, 2].imshow(log_mag_FFT(image2), vmin=0, vmax=10)\n",
        "      axs[1, 3].imshow(log_mag_FFT(high_frequencies), vmin=0, vmax=10)\n",
        "      axs[1, 4].imshow(log_mag_FFT(output), vmin=0, vmax=10)\n",
        "\n",
        "      axs[0, 0].set_title('Image 1')\n",
        "      axs[0, 1].set_title('Low-freq of Image 1')\n",
        "      axs[0, 2].set_title('Image 2')\n",
        "      axs[0, 3].set_title('High-freq of Image 2')\n",
        "      axs[0, 4].set_title('Hybrid image')\n",
        "      axs[1, 0].set_title('FFT of Image 1')\n",
        "      axs[1, 1].set_title('FFT of Low-freq Image 1')\n",
        "      axs[1, 2].set_title('FFT of Image 2')\n",
        "      axs[1, 3].set_title('FFT of High-freq Image 2')\n",
        "      axs[1, 4].set_title('FFT of Hybrid image')\n",
        "      #plt.subplots_adjust(hspace=0)\n",
        "      fig.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "      # visualize hybrid_image_scale using helper function vis_hybrid_image.\n",
        "      fig, ax = plt.subplots(figsize=(20, 10))\n",
        "      ax.imshow(vis_hybrid_image(output))\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    return output  # Ensure valid pixel range for floating point images (0-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-BYC-xNQIhU"
      },
      "source": [
        "Let's see if we are doing it right. You should see a hybrid image of a dog and a cat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSzP9gzTQIJ5"
      },
      "outputs": [],
      "source": [
        "# read two images\n",
        "image_path1 = '/content/data/dog.bmp'\n",
        "image_path2 = '/content/data/cat.bmp'\n",
        "image1 = read_image(image_path1)\n",
        "image2 = read_image(image_path2)\n",
        "\n",
        "# do hybrid image\n",
        "hybrid_output = hybrid_image(image1, image2, sigma_low_freq=10, sigma_high_freq=2, if_visual=False)\n",
        "\n",
        "# show output\n",
        "plt.imshow(hybrid_output)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmN8MtK2mOUz"
      },
      "source": [
        "### Write-up (10 pts)\n",
        "\n",
        "\n",
        "1.   Run `hybrid_image` on 3 different combinations. Report your results below. (5 pts) NOTE: You should always compose images that have the same shape.\n",
        "3.   What happened to the FFTs? Anything changed? Explain what you have found. (2 pts)\n",
        "4.   Briefly explain how this works, using your favorite results as illustrations. (3 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I06DhqYI4YI7"
      },
      "source": [
        "**Include your write-up here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WkvRx5uFPQP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J101muTWnukO"
      },
      "source": [
        "# Save as a PDF file\n",
        "Make sure you have run Dependecies below. It will take a while.\n",
        "You will find PDF file on your left hand side.\n",
        "If the following code does not work for you, go `File > Print > Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nt9OikP8oOkZ"
      },
      "outputs": [],
      "source": [
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_cVWqZGpF1b"
      },
      "source": [
        "You will be prompted to allow permissions to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-bmpJYzl7UU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOuq4f4PmkbE"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/CMSC426-Spring25/assignments/CMSC426_Assignment1_sp26.ipynb ./\n",
        "!jupyter nbconvert --to PDF \"CMSC426_Assignment1_sp26.ipynb\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1oV0XsPm7jx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}